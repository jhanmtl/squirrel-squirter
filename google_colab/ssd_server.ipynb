{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ssd_server.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TthGstWBC3G1",
        "colab_type": "text"
      },
      "source": [
        "created by j.han: tfnotebooks@gmail.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysN4T1F91XCf",
        "colab_type": "text"
      },
      "source": [
        "# params and directories\n",
        "\n",
        "this notebook assumes the following directory structure:\n",
        "\n",
        "/content\n",
        "\n",
        "    |--- /gdrive\n",
        "            |--- /My Drive\n",
        "                    |--- /TensorFlow\n",
        "                            |---/frozen_models\n",
        "                                    |---/ssd\n",
        "                                          |---your frozen model.pb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iAwxOpw1iS6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pb_path='_______________________________'             #name of your frozen model.pb upload it to according to the directory shown above\n",
        "link='_________________________'                      #the forward url from ngrok that captures the video stream \n",
        "threshold=0.5                                         #prediction thereshold\n",
        "authtoken = '_______________________' \n",
        "#authtoken for the ngrok account used to forward this machine. note that this is a separate account from the one used to forward the raspberry pi videostream"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeimpqBYThu8",
        "colab_type": "text"
      },
      "source": [
        "# checking GPU specs\n",
        "T4 would be the best, but unlikely to get since Colab frowns upon this kind of usage for its gpu. Most likely will get K80, still decent enough \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akZLuh-aPyzP",
        "colab_type": "code",
        "outputId": "30aaed1e-7cbc-42c3-92d2-d550194830ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "!nvidia-smi\n",
        "!/usr/local/cuda/bin/nvcc --version"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Sep 14 00:13:45 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 430.40       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P8    29W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2018 NVIDIA Corporation\n",
            "Built on Sat_Aug_25_21:08:01_CDT_2018\n",
            "Cuda compilation tools, release 10.0, V10.0.130\n",
            "Sat Sep 14 00:49:08 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 430.40       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P0    60W / 149W |   3306MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2018 NVIDIA Corporation\n",
            "Built on Sat_Aug_25_21:08:01_CDT_2018\n",
            "Cuda compilation tools, release 10.0, V10.0.130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1vyMKU6ULYf",
        "colab_type": "text"
      },
      "source": [
        "# ngrok tunneling setup\n",
        "\n",
        "credit to [Imad El-Hanafi](https://imadelhanafi.com/posts/google_colal_server/)\n",
        "for his great work on SSH into the Colab virtual machines\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqEY-6Ff-Ir-",
        "colab_type": "code",
        "outputId": "9cf60457-1051-44e0-d7ce-0e39a638dc92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "#mounts the drive\n",
        "from google.colab import drive                                                          \n",
        "drive.mount('/content/gdrive/')\n",
        "\n",
        "#sets up ssh\n",
        "import random, string\n",
        "password = ''.join(random.choice(string.ascii_letters + string.digits) for i in range(5))\n",
        "! apt-get install -qq -o=Dpkg::Use-Pty=0 openssh-server pwgen > /dev/null\n",
        "! echo root:$password | chpasswd\n",
        "! mkdir -p /var/run/sshd\n",
        "! echo \"PermitRootLogin yes\" >> /etc/ssh/sshd_config\n",
        "! echo \"PasswordAuthentication yes\" >> /etc/ssh/sshd_config\n",
        "get_ipython().system_raw('/usr/sbin/sshd -D &')\n",
        "! wget -q -c -nc https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "! unzip -qq -n ngrok-stable-linux-amd64.zip\n",
        "\n",
        "# opens a tcp tunnel with ngrok so that the terminal of the virtual machine that this colab session is running on can be remotely accessed\n",
        "get_ipython().system_raw('./ngrok authtoken $authtoken && ./ngrok tcp 22 &')\n",
        "print('root')\n",
        "print(password)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n",
            "root\n",
            "g9cch\n",
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n",
            "root\n",
            "kqmIU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY5ZF0zBD34u",
        "colab_type": "text"
      },
      "source": [
        "now go to this [link](https://dashboard.ngrok.com/status) to open up the ngrok dashboard. note the port number.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdsV_zZSEMPL",
        "colab_type": "text"
      },
      "source": [
        "# Change directories\n",
        "navigate to the same directory as uploaded frozen model.pb\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0ZM31D80-QR",
        "colab_type": "code",
        "outputId": "21682789-a3aa-4b5e-fc40-45d016206e85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "%cd ~\n",
        "%cd /content/gdrive/'My Drive'/TensorFlow/frozen_models/ssd"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n",
            "/content/gdrive/My Drive/TensorFlow/frozen_models/ssd\n",
            "/root\n",
            "/content/gdrive/My Drive/TensorFlow/frozen_models/ssd\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sy53ZTwm1HaM",
        "colab_type": "text"
      },
      "source": [
        "# frameGrabber class\n",
        "\n",
        "since the video streamed is acutally MJPEGs, the default opencv videocapture class wouldn't work. Instead, use GET method from the requests module to pull the image bytes directly. Multi-threaded for reduced I/O latency. Inspired by Zaw Lin's answer in this [stackoverflow post](https://stackoverflow.com/questions/21702477/how-to-parse-mjpeg-http-stream-from-ip-camera/21844162)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcwxtqTZ0uR9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from threading import Thread  \n",
        "import cv2\n",
        "import requests\n",
        "import numpy as np\n",
        "\n",
        "class frameGrabber:\n",
        "    def __init__(self,url):\n",
        "        self.r=requests.get(url,stream=True)                                    #GET method to the video streaming url\n",
        "        self.img=None                                                           #current image\n",
        "        self.stopped=False                                                      #whether to stop grabbing\n",
        "        print(self.r.status_code)                                               #200 is good  \n",
        "                  \n",
        "        \n",
        "    def start(self):\n",
        "        print('starting thread')\n",
        "        t=Thread(target=self.update,args=())                                  \n",
        "        t.setDaemon(True)\n",
        "        t.start()\n",
        "        print('thread started')\n",
        "        return self\n",
        "       \n",
        "    def update(self):\n",
        "        while True:\n",
        "            \n",
        "            if self.stopped==True:\n",
        "                return\n",
        "            else:\n",
        "                img_bytes=bytes()\n",
        "                for chunk in self.r.iter_content(chunk_size=1024):\n",
        "                    img_bytes+=chunk\n",
        "                    a=img_bytes.find(b'\\xff\\xd8')                               #jpg on page always starts with these bytes\n",
        "                    b=img_bytes.find(b'\\xff\\xd9')                               #jpg on page always ends with these bytes\n",
        "                    if a!=-1 and b!=-1:\n",
        "                        jpg=img_bytes[a:b+2]\n",
        "                        img_bytes=img_bytes[b+2:]\n",
        "                        self.img=cv2.imdecode(np.frombuffer(jpg,dtype=np.uint8),cv2.IMREAD_COLOR) #byts to np array\n",
        "    def read(self):\n",
        "        return self.img\n",
        "    \n",
        "    def stop(self):\n",
        "        self.stopped=True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOgLCnGw14g5",
        "colab_type": "text"
      },
      "source": [
        "# model\n",
        "\n",
        "standard setup of tf model\n",
        "\n",
        "model outputs '<x1,y1,x2,y2,class label,confidence,fps>' upon detection "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkTdB3vJKM0D",
        "colab_type": "code",
        "outputId": "e521fc23-1ee5-42e6-cf39-9c7a05b73a34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "from flask import Flask, Response\n",
        "import tensorflow as tf\n",
        "import cv2 \n",
        "import numpy as np\n",
        "import time\n",
        "import argparse\n",
        "import sys\n",
        "\n",
        "def predict(img,sess,label_map):\n",
        "    rows = img.shape[0]\n",
        "    cols = img.shape[1]\n",
        "    img_expanded=np.expand_dims(img,axis=0)\n",
        "    \n",
        "    start=time.time()\n",
        "    out = sess.run([sess.graph.get_tensor_by_name('num_detections:0'),\n",
        "                    sess.graph.get_tensor_by_name('detection_scores:0'),\n",
        "                    sess.graph.get_tensor_by_name('detection_boxes:0'),\n",
        "                    sess.graph.get_tensor_by_name('detection_classes:0')],\n",
        "                   feed_dict={'image_tensor:0': img_expanded})\n",
        "    elapsed=time.time()-start\n",
        "    if elapsed>0:\n",
        "          fps=int(1/elapsed)\n",
        "        \n",
        "    top_five=out[1][0][0:5]\n",
        "    top_five=[round(100*i,2) for i in top_five]\n",
        "     \n",
        "    classId = int(out[3][0][0])\n",
        "    score = float(out[1][0][0])\n",
        "    bbox = [float(v) for v in out[2][0][0]]\n",
        "    if score > threshold:\n",
        "        x = bbox[1] * cols\n",
        "        y = bbox[0] * rows\n",
        "        right = bbox[3] * cols\n",
        "        bottom = bbox[2] * rows\n",
        "        label=label_map[classId]\n",
        "        info='<'+str(round(x))+','+str(round(y))+','+str(round(right))+','+str(round(bottom))+','+str(label)+','+str(round(score,2))+','+str(fps)+'>'\n",
        "        return info\n",
        "    else:\n",
        "      return '<pass>'\n",
        "\n",
        "\n",
        "label_map= {0: 'background',\n",
        "              1: 'person', 2: 'bicycle', 3: 'car', 4: 'motorcycle', 5: 'airplane', 6: 'bus',\n",
        "              7: 'train', 8: 'truck', 9: 'boat', 10: 'traffic light', 11: 'fire hydrant',\n",
        "              13: 'stop sign', 14: 'parking meter', 15: 'bench', 16: 'bird', 17: 'cat',\n",
        "              18: 'dog', 19: 'horse', 20: 'sheep', 21: 'cow', 22: 'elephant', 23: 'bear',\n",
        "              24: 'zebra', 25: 'giraffe', 27: 'backpack', 28: 'umbrella', 31: 'handbag',\n",
        "              32: 'tie', 33: 'suitcase', 34: 'frisbee', 35: 'skis', 36: 'snowboard',\n",
        "              37: 'sports ball', 38: 'kite', 39: 'baseball bat', 40: 'baseball glove',\n",
        "              41: 'skateboard', 42: 'surfboard', 43: 'tennis racket', 44: 'bottle',\n",
        "              46: 'wine glass', 47: 'cup', 48: 'fork', 49: 'knife', 50: 'spoon',\n",
        "              51: 'bowl', 52: 'banana', 53: 'apple', 54: 'sandwich', 55: 'orange',\n",
        "              56: 'broccoli', 57: 'carrot', 58: 'hot dog', 59: 'pizza', 60: 'donut',\n",
        "              61: 'cake', 62: 'chair', 63: 'couch', 64: 'potted plant', 65: 'bed',\n",
        "              67: 'dining table', 70: 'toilet', 72: 'tv', 73: 'laptop', 74: 'mouse',\n",
        "              75: 'remote', 76: 'keyboard', 77: 'cell phone', 78: 'microwave', 79: 'oven',\n",
        "              80: 'toaster', 81: 'sink', 82: 'refrigerator', 84: 'book', 85: 'clock',\n",
        "              86: 'vase', 87: 'scissors', 88: 'teddy bear', 89: 'hair drier', 90: 'toothbrush'}\n",
        "\n",
        "cap=frameGrabber(link).start()\n",
        "\n",
        "detection_graph = tf.Graph()\n",
        "with detection_graph.as_default():\n",
        "    od_graph_def = tf.compat.v1.GraphDef()\n",
        "    with tf.io.gfile.GFile(pb_path, 'rb') as fid:\n",
        "        serialized_graph = fid.read()\n",
        "        od_graph_def.ParseFromString(serialized_graph)\n",
        "        tf.import_graph_def(od_graph_def, name='')\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200\n",
            "starting thread\n",
            "thread started\n",
            "404\n",
            "starting thread\n",
            "thread started\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception in thread Thread-6:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 864, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"<ipython-input-12-417be9e40739>\", line 29, in update\n",
            "    for chunk in self.r.iter_content(chunk_size=1024):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/requests/models.py\", line 769, in iter_content\n",
            "    raise StreamConsumedError()\n",
            "requests.exceptions.StreamConsumedError\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HL94kOy2ehd",
        "colab_type": "text"
      },
      "source": [
        "# web server\n",
        "\n",
        "starts web service to upload each frame's prediction information at port 5000 of the local network.\n",
        "the uploaded info is either  '<x1,y1,x2,y2,class label,confidence,fps>' or pass otherwise\n",
        "\n",
        "This info is NOT accessible until start third ngrok session. See cell below on instruction\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSEyeztzEtX-",
        "colab_type": "code",
        "outputId": "3f11ecad-d3c3-4742-c3a8-b2e48dbc9082",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/')\n",
        "def index():\n",
        "    return Response(gen(detection_graph,label_map),\n",
        "                    mimetype='multipart/x-mixed-replace; boundary=frame')\n",
        "\n",
        "def gen(detection_graph,label_map):\n",
        "    print('start prediction')\n",
        "    print()\n",
        "\n",
        "    with detection_graph.as_default():\n",
        "        with tf.compat.v1.Session(graph=detection_graph) as sess:\n",
        "            while True:\n",
        "#                 start=time.time()\n",
        "                img=cap.read()\n",
        "                info=predict(img,sess,label_map)\n",
        "                if info is not None:\n",
        "                  yield (info)\n",
        "                else:\n",
        "                  yield ('<pass>')\n",
        "\n",
        "app.run()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "   WARNING: This is a development server. Do not use it in a production deployment.\n",
            "   Use a production WSGI server instead.\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "start prediction\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [14/Sep/2019 00:15:25] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "Exception in thread Thread-4:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 864, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"<ipython-input-5-417be9e40739>\", line 29, in update\n",
            "    for chunk in self.r.iter_content(chunk_size=1024):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/requests/models.py\", line 769, in iter_content\n",
            "    raise StreamConsumedError()\n",
            "requests.exceptions.StreamConsumedError\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "   WARNING: This is a development server. Do not use it in a production deployment.\n",
            "   Use a production WSGI server instead.\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJYr_uo5RqGW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# go to https://dashboard.ngrok.com/status\n",
        "# note the port number (ex for 0.tcp.ngrok.io: 17129, port number would be 17129)\n",
        "# (linux, iOS): use ssh at terminal to connect to the remote machine's command line via the username and password generated in the ngrok tunneling stup section\n",
        "# (winodws): use PuTTY with the same username and password\n",
        "\n",
        "# type the following:\n",
        "# cd /content\n",
        "# ./ngrok authtoken (paste the authtoken from your 3rd ngrok account)\n",
        "# ./ngrok http 5000\n",
        "\n",
        "# you will now see something like below\n",
        "# keep this window open and note the http://*****.ngrok.io/  address that the colab server is forwarded to\n",
        "# now go to the local machine and run client.py with the address \n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}